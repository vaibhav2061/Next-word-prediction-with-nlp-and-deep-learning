{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of SOS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDNlsrMd3YFl"
      },
      "source": [
        " %tensorflow_version 2.x\n",
        " import tensorflow as tf\n",
        " import string\n",
        " import requests\n",
        " "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVhl3elo37tR"
      },
      "source": [
        "response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4_o6XZc4X_F"
      },
      "source": [
        "response.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDHSkTE-4bCn"
      },
      "source": [
        "data = response.text.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf_ivZJEdGao"
      },
      "source": [
        "data  = \" \".join(data)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWZK24RLdGfQ"
      },
      "source": [
        "def clean_text(doc):\n",
        "  tokens = doc.split()\n",
        "  table  = str.maketrans('','',string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxcEPhwmdGg8",
        "outputId": "cb16cbc9-9baa-4df1-d734-839056ea8f56"
      },
      "source": [
        "tokens  = clean_text(data)\n",
        "print(tokens[:50])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'is', 'the', 'etext', 'file', 'presented', 'by', 'project', 'gutenberg', 'and', 'is', 'presented', 'in', 'cooperation', 'with', 'world', 'library', 'inc', 'from', 'their', 'library', 'of', 'the', 'future', 'and', 'shakespeare', 'cdroms', 'project', 'gutenberg', 'often', 'releases', 'etexts', 'that', 'are', 'not', 'placed', 'in', 'the', 'public', 'domain', 'shakespeare', 'this', 'etext', 'has', 'certain', 'copyright', 'implications', 'you', 'should', 'read']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKGh_lBLdGjX",
        "outputId": "629c3831-0088-40b1-c157-87fe4ec973c3"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "899788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nd0iIcYdGlI",
        "outputId": "12708a0b-c62c-46a1-dc79-a2f1ad0a1535"
      },
      "source": [
        "len(set(tokens))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28113"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sm5IiuvdGpE"
      },
      "source": [
        " length  = 50 + 1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyY6uTzFfWqc",
        "outputId": "9a7be3ad-c3dd-4263-bb84-0e34b61144e6"
      },
      "source": [
        "lines  = []\n",
        "for i in range(length,len(tokens)):\n",
        "  seq = tokens[i-length:i]\n",
        "  line  = ' '.join(seq)\n",
        "  lines.append(line)\n",
        "  if i > 100000:\n",
        "    break\n",
        "\n",
        "print(len(lines))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5jCYUMai854"
      },
      "source": [
        "## Buliding LSTM Model and prepare `x` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NZy5QX2jDjm"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVih1t9Mjqdg"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxLXvzslm-ID"
      },
      "source": [
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1],sequences[:,-1]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu0L6iwQoftF"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z2ARoMjm-1s"
      },
      "source": [
        "y = to_categorical(y,num_classes=vocab_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMs_Lx7mjqfG"
      },
      "source": [
        "seq_length = X.shape[1]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOlSo4vBpVCR"
      },
      "source": [
        "##LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WTqkNh0jqhy"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "model.add(LSTM(100,return_sequences =True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dense(vocab_size,activation='softmax'))\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhg1Y06ojqmL"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuFAvSJWuAxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab48826-fc38-4436-bc43-41435d7a1cf9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            450750    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 9015)              910515    \n",
            "=================================================================\n",
            "Total params: 1,512,165\n",
            "Trainable params: 1,512,165\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml3naDMFjqnk",
        "outputId": "5a3a5164-490b-424c-c654-9b3ca559f61e"
      },
      "source": [
        "model.fit(X,y,batch_size=256,epochs=200)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "391/391 [==============================] - 13s 27ms/step - loss: 6.9432 - accuracy: 0.0280\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 6.5849 - accuracy: 0.0328\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 6.4304 - accuracy: 0.0402\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 6.3126 - accuracy: 0.0453\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 6.1973 - accuracy: 0.0553\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 6.0790 - accuracy: 0.0632\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.9722 - accuracy: 0.0696\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.9216 - accuracy: 0.0728\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 5.8336 - accuracy: 0.0791\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 5.7389 - accuracy: 0.0855\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 5.6583 - accuracy: 0.0914\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 5.5813 - accuracy: 0.0983\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.5022 - accuracy: 0.1028\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.4161 - accuracy: 0.1073\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.3390 - accuracy: 0.1103\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.2592 - accuracy: 0.1135\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 5.2067 - accuracy: 0.1145\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 5.1421 - accuracy: 0.1168\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.0723 - accuracy: 0.1184\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 5.0093 - accuracy: 0.1205\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.9336 - accuracy: 0.1233\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.8536 - accuracy: 0.1268\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.7785 - accuracy: 0.1308\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.7092 - accuracy: 0.1351\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.6642 - accuracy: 0.1400\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.6299 - accuracy: 0.1425\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.5749 - accuracy: 0.1471\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.5441 - accuracy: 0.1501\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.4648 - accuracy: 0.1573\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.4244 - accuracy: 0.1613\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.3971 - accuracy: 0.1638\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.3248 - accuracy: 0.1706\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.2828 - accuracy: 0.1766\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.2668 - accuracy: 0.1779\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.2201 - accuracy: 0.1840\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.1777 - accuracy: 0.1883\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.1472 - accuracy: 0.1921\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.1145 - accuracy: 0.1958\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 4.0889 - accuracy: 0.2000\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 4.0412 - accuracy: 0.2032\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.9998 - accuracy: 0.2091\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.9687 - accuracy: 0.2127\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.9319 - accuracy: 0.2181\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.8820 - accuracy: 0.2229\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.8775 - accuracy: 0.2240\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.8205 - accuracy: 0.2319\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.7948 - accuracy: 0.2349\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.7617 - accuracy: 0.2396\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.7244 - accuracy: 0.2439\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.7268 - accuracy: 0.2452\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.6950 - accuracy: 0.2484\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.7313 - accuracy: 0.2467\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.7667 - accuracy: 0.2424\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.6387 - accuracy: 0.2567\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.6115 - accuracy: 0.2622\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.5464 - accuracy: 0.2699\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.5108 - accuracy: 0.2756\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.5265 - accuracy: 0.2743\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.6309 - accuracy: 0.2631\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.6232 - accuracy: 0.2624\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.6984 - accuracy: 0.2528\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.6738 - accuracy: 0.2550\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.6737 - accuracy: 0.2522\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.5365 - accuracy: 0.2723\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.4914 - accuracy: 0.2790\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.4569 - accuracy: 0.2851\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.5080 - accuracy: 0.2788\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.4680 - accuracy: 0.2838\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.4322 - accuracy: 0.2889\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.4421 - accuracy: 0.2879\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.4540 - accuracy: 0.2857\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.4021 - accuracy: 0.2939\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.3629 - accuracy: 0.2984\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.3353 - accuracy: 0.3031\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.2937 - accuracy: 0.3091\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.3273 - accuracy: 0.3031\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.2284 - accuracy: 0.3166\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.1800 - accuracy: 0.3254\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.1552 - accuracy: 0.3284\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.1411 - accuracy: 0.3315\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.1171 - accuracy: 0.3356\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.1726 - accuracy: 0.3289\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.1048 - accuracy: 0.3386\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.1449 - accuracy: 0.3347\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.7490 - accuracy: 0.2592\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.9676 - accuracy: 0.2151\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.7519 - accuracy: 0.2370\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.6441 - accuracy: 0.2534\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.5666 - accuracy: 0.2636\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 3.5130 - accuracy: 0.2704\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.4587 - accuracy: 0.2788\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.4098 - accuracy: 0.2876\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.3676 - accuracy: 0.2938\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 3.3248 - accuracy: 0.2990\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.2839 - accuracy: 0.3070\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.2462 - accuracy: 0.3128\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.2053 - accuracy: 0.3193\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.1693 - accuracy: 0.3241\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.1309 - accuracy: 0.3317\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.0975 - accuracy: 0.3369\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 3.0631 - accuracy: 0.3427\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 3.0325 - accuracy: 0.3477\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 3.0019 - accuracy: 0.3524\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.9649 - accuracy: 0.3586\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.9362 - accuracy: 0.3638\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.9063 - accuracy: 0.3701\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.8730 - accuracy: 0.3747\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.8470 - accuracy: 0.3790\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.8169 - accuracy: 0.3858\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.7841 - accuracy: 0.3905\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.7553 - accuracy: 0.3953\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.7276 - accuracy: 0.4006\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.7032 - accuracy: 0.4044\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.6758 - accuracy: 0.4118\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.6446 - accuracy: 0.4150\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.6222 - accuracy: 0.4183\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.5932 - accuracy: 0.4239\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.5657 - accuracy: 0.4289\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.5409 - accuracy: 0.4347\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.5166 - accuracy: 0.4390\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.4937 - accuracy: 0.4438\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.4647 - accuracy: 0.4485\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.4414 - accuracy: 0.4511\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.4146 - accuracy: 0.4577\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.3942 - accuracy: 0.4603\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.3697 - accuracy: 0.4655\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.3423 - accuracy: 0.4716\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.3175 - accuracy: 0.4758\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.2920 - accuracy: 0.4805\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.2708 - accuracy: 0.4843\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.2463 - accuracy: 0.4892\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.2294 - accuracy: 0.4939\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.2064 - accuracy: 0.4968\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.1890 - accuracy: 0.5004\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 2.1619 - accuracy: 0.5047\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.1430 - accuracy: 0.5098\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.1145 - accuracy: 0.5153\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.0892 - accuracy: 0.5206\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.0699 - accuracy: 0.5231\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.0525 - accuracy: 0.5277\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.0316 - accuracy: 0.5305\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 2.0056 - accuracy: 0.5371\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.9931 - accuracy: 0.5386\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.9756 - accuracy: 0.5421\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.9512 - accuracy: 0.5467\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.9296 - accuracy: 0.5506\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.9138 - accuracy: 0.5547\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.8915 - accuracy: 0.5585\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.8749 - accuracy: 0.5624\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.8516 - accuracy: 0.5679\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.8336 - accuracy: 0.5701\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.8130 - accuracy: 0.5750\n",
            "Epoch 153/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.7979 - accuracy: 0.5775\n",
            "Epoch 154/200\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 1.7825 - accuracy: 0.5816\n",
            "Epoch 155/200\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 1.7648 - accuracy: 0.5851\n",
            "Epoch 156/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.7424 - accuracy: 0.5900\n",
            "Epoch 157/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.7252 - accuracy: 0.5933\n",
            "Epoch 158/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.7069 - accuracy: 0.5984\n",
            "Epoch 159/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.6926 - accuracy: 0.5987\n",
            "Epoch 160/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.6775 - accuracy: 0.6024\n",
            "Epoch 161/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.6524 - accuracy: 0.6091\n",
            "Epoch 162/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.6403 - accuracy: 0.6110\n",
            "Epoch 163/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.6261 - accuracy: 0.6134\n",
            "Epoch 164/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.6071 - accuracy: 0.6181\n",
            "Epoch 165/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.5892 - accuracy: 0.6215\n",
            "Epoch 166/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.5697 - accuracy: 0.6249\n",
            "Epoch 167/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.5691 - accuracy: 0.6240\n",
            "Epoch 168/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.5471 - accuracy: 0.6301\n",
            "Epoch 169/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.5260 - accuracy: 0.6350\n",
            "Epoch 170/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.5067 - accuracy: 0.6399\n",
            "Epoch 171/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4997 - accuracy: 0.6404\n",
            "Epoch 172/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4786 - accuracy: 0.6457\n",
            "Epoch 173/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4675 - accuracy: 0.6485\n",
            "Epoch 174/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.4567 - accuracy: 0.6487\n",
            "Epoch 175/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4351 - accuracy: 0.6555\n",
            "Epoch 176/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4219 - accuracy: 0.6581\n",
            "Epoch 177/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4088 - accuracy: 0.6606\n",
            "Epoch 178/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.4048 - accuracy: 0.6600\n",
            "Epoch 179/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.3902 - accuracy: 0.6646\n",
            "Epoch 180/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.3651 - accuracy: 0.6698\n",
            "Epoch 181/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.3562 - accuracy: 0.6734\n",
            "Epoch 182/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.3387 - accuracy: 0.6767\n",
            "Epoch 183/200\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 1.3342 - accuracy: 0.6766\n",
            "Epoch 184/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.3216 - accuracy: 0.6785\n",
            "Epoch 185/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.3080 - accuracy: 0.6817\n",
            "Epoch 186/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.2937 - accuracy: 0.6855\n",
            "Epoch 187/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.2812 - accuracy: 0.6880\n",
            "Epoch 188/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.2593 - accuracy: 0.6937\n",
            "Epoch 189/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.2545 - accuracy: 0.6938\n",
            "Epoch 190/200\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 1.2383 - accuracy: 0.6976\n",
            "Epoch 191/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.2362 - accuracy: 0.6980\n",
            "Epoch 192/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.2117 - accuracy: 0.7042\n",
            "Epoch 193/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.1992 - accuracy: 0.7071\n",
            "Epoch 194/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.2014 - accuracy: 0.7063\n",
            "Epoch 195/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.1921 - accuracy: 0.7076\n",
            "Epoch 196/200\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 1.1882 - accuracy: 0.7062\n",
            "Epoch 197/200\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 1.1670 - accuracy: 0.7131\n",
            "Epoch 198/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.1530 - accuracy: 0.7172\n",
            "Epoch 199/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.1322 - accuracy: 0.7215\n",
            "Epoch 200/200\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 1.1238 - accuracy: 0.7238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f06bdf4be90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaHFm0oz0jM5"
      },
      "source": [
        "seed_text = lines[123]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfKR0QHUMsvW"
      },
      "source": [
        "def generate_text_seq(model,tokenizer,text_seq_length,seed_text,n_words):\n",
        "  text = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=text_seq_length,truncating='pre')\n",
        "\n",
        "    y_predic = model.predict_classes(encoded)\n",
        "\n",
        "    predicted_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predic:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text + ' ' +predicted_word\n",
        "    text.append(predicted_word)\n",
        "  return ' '.join(text)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQdIVC7ksifw"
      },
      "source": [
        "generate_text_seq(model,tokenizer,seq_length,seed_text,10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}